The below code trains a neural network on a dataset containing different product assortment combinations and their respective sales percentages. The neural network is then used as a fixed effect in a mixed model, where the neural network is used to predict the sales percentages.

```{r}
library(lme4)
library(tensorflow)
library(keras)
library(kerastuneR)
library(tidyverse)
```

```{r}
data <- read.csv("data/historical-data-2023.csv", header = FALSE) %>%
    mutate(V1 = as.numeric(V1)) %>%
    mutate_all(~ ifelse(is.na(.), 0, .))
# this is bacause there is a bug, causing the observation at location 1,1
# to turn into NA. As I know this is a zero, I can just replace it with zero.

# extract the product percentages and assortment availability
output_data <- data %>%
    subset(seq(nrow(data)) %% 2 == 0) %>%
    as.matrix()

input_data <- data %>%
    subset(seq(nrow(data)) %% 2 != 0) %>%
    as.matrix()

set.seed(1234)
df <- cbind(input_data, output_data)
train_idx <- sample(nrow(df), nrow(df) * 0.7)

train_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

train_input <- train_data[, 1:ncol(input_data)]
train_output <- train_data[, (ncol(input_data) + 1):ncol(df)]

test_input <- test_data[, 1:ncol(input_data)]
test_output <- test_data[, (ncol(input_data) + 1):ncol(df)]

```

```{r}
tuner <- Hyperband(
    hypermodel = function(hp) {
        model <- keras_model_sequential()
        model %>%
            layer_dense(
                units = hp$Int("units_1", 32, 256, step = 32),
                activation = hp$Choice("activation_1", c("relu", "tanh", "sigmoid")),
                input_shape = c(15)
            ) %>%
            layer_dense(
                units = hp$Int("units_2", 16, 128, step = 16),
                activation = hp$Choice("activation_2", c("relu", "tanh", "sigmoid"))
            ) %>%
            layer_dense(units = 15, activation = "softmax")

        model %>% compile(
            loss = "categorical_crossentropy", # can also do "mean_squared_error", but not sure if appropriate for a discrete choice model
            optimizer = hp$Choice("optimizer", c("adam", "rmsprop")),
            metrics = list("accuracy", "mse", "mae")
        )

        return(model)
    },
    objective = "accuracy", # can also do val_mse, but not sure if appropriate
    max_epochs = 50,
    factor = 3
)

mixed_model <- glmer(output ~ (1 | subject) + neural_network, 
                     data = train_data,
                     family = binomial())

tuner %>%
    fit_tuner(
        x = train_input,
        y = mixed_model,
        validation_split = 0.2,
        epochs = 100,
        batch_size = 32
    )
```